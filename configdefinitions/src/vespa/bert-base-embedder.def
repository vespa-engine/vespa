# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.

namespace=embedding

# Wordpiece tokenizer
tokenizerVocab model

transformerModel  model

# Max length of token sequence model can handle
transformerMaxTokens int default=384

# Pooling strategy
poolingStrategy enum { cls, mean } default=mean

# Input names
transformerInputIds      string default=input_ids
transformerAttentionMask string default=attention_mask
transformerTokenTypeIds  string default=token_type_ids

# special token ids
transformerStartSequenceToken int default=101
transformerEndSequenceToken   int default=102

# Output name
transformerOutput string default=output_0

# Settings for ONNX model evaluation
onnxExecutionMode enum { parallel, sequential } default=sequential
onnxInterOpThreads int default=1
onnxIntraOpThreads int default=-4  # n=number of threads -> n<0: CPUs/(-n), n==0: CPUs, n>0: n
# GPU device id, -1 for CPU
onnxGpuDevice      int default=0

# Controls dynamic batching that combines up to `maxSize` requests within `maxDelayMillis` into a single inference call.
# This increases throughput at the cost of latency.
# This is especially effective for generating embeddings during feeding on GPU.
# `maxDelayMillis` is specified in milliseconds.
# The default settings disables dynamic batching.
batching.maxSize int default=1
batching.maxDelayMillis long default=1

# Concurrency creates several model instances for concurrent inference.
# This increases throughput without affecting latency at the cost of memory used by each model instance.
# This is especially effective for embedding queries on multi-core CPU.
# With `factorType=absolute`, `factor` is rounded to set the number of model instances.
# With `factorType=relative`, `factor` is multiplied with the number of CPU cores and rounded to set the number of model instances.
# Default settings creates a single model instance.
concurrency.factorType enum { absolute, relative } default=absolute
concurrency.factor double default=1.0

# Path to model config file, e.g. `config.pbtxt` used by Triton inference server.
modelConfigOverride path optional