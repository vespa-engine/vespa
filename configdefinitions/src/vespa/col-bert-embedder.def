# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.

namespace=embedding

# Path to tokenizer.json
tokenizerPath model

# Path to model.onnx
transformerModel  model

# Max query tokens for ColBERT
maxQueryTokens int default=32

# Max document query tokens for ColBERT
maxDocumentTokens int default=512

# Max length of token sequence model can handle
transformerMaxTokens int default=512

# Input names
transformerInputIds      string default=input_ids
transformerAttentionMask string default=attention_mask

# special token ids
transformerStartSequenceToken int default=101
transformerEndSequenceToken   int default=102
transformerMaskToken   int default=103
transformerPadToken    int default=0
queryTokenId     int default=1
documentTokenId  int default=2

# Output name
transformerOutput string default=contextual

# Settings for ONNX model evaluation
transformerExecutionMode enum { parallel, sequential } default=sequential
transformerInterOpThreads int default=1
transformerIntraOpThreads int default=-4
# GPU device id, -1 for CPU
transformerGpuDevice      int default=0

# Controls dynamic batching that combines up to `maxSize` requests within `maxDelayMillis` into a single inference call.
# This increases throughput at the cost of latency.
# This is especially effective for generating embeddings during feeding on GPU.
# `maxDelayMillis` is specified in milliseconds.
# The default settings disables dynamic batching.
batching.maxSize int default=1
batching.maxDelayMillis long default=1

# Concurrency creates several model instances for concurrent inference.
# This increases throughput without affecting latency at the cost of memory used by each model instance.
# This is especially effective for embedding queries on multi-core CPU.
# With `factorType=absolute`, `factor` is rounded to set the number of model instances.
# With `factorType=relative`, `factor` is multiplied with the number of CPU cores and rounded to set the number of model instances.
# Default settings creates a single model instance.
concurrency.factorType enum { absolute, relative } default=absolute
concurrency.factor double default=1.0

# Path to model config file, e.g. `config.pbtxt` used by Triton inference server.
modelConfigOverride path optional