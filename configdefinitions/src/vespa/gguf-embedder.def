package=ai.vespa.embedding.config

# GGUF embedding model
embeddingModel model

# Maximum number of model layers to run on GPU (default: 0 = CPU-only).
gpuLayers int default=0

# Set pooling type for embeddings (default: UNSPECIFIED, UNSPECIFIED = model default).
poolingType enum {UNSPECIFIED, NONE, MEAN, CLS, LAST, RANK} default=UNSPECIFIED

# Set the physical maximum batch size (default: -1 = use runtime default).
physicalMaxBatchSize int default=-1

# Set the logical maximum batch size (default: -1 = use runtime default).
logicalMaxBatchSize int default=-1

# Continuous batching mode (default: false).
continuousBatching bool default=false

# Set the size of the prompt context (default: 0 = loaded from model).
contextSize int default=0

# Set the number of tokens to keep from the initial prompt (default: 0 = all)
maxPromptTokens int default=0

# Random number generator seed used by LLM runtime during inference (default: -1 = use random seed).
seed int default = -1
