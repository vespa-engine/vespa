# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
package=ai.vespa.llm.generation

# Id of a LanguageModel component specified in services.xml, e.g. OpenAI, LocalLLM.
providerId string

# Prompt template.
# It may contain {input} and optionally {jsonSchema} placeholders.
# Placeholder {input} will be replaced with the input text from the indexing statement.
# If responseFormatType is JSON, placeholder {jsonSchema} will be replaced with the JSON schema 
# specified in responseJsonSchema, responseJsonSchemaFile or generated from the target field type.
promptTemplate string default=""

# Path to a text file containing prompt template.
# It is used when promptTemplate is not set or empty.
# The content of the file should have the same format as promptTemplate.
# The path is relative to the application package root where services.xml is located.
promptTemplateFile path optional

# Format for LLM response.
# JSON - structured output in JSON format according to JSON schema generated based on the target field type. 
# JSON is the recommended default because it reduces hallucinations.
# TEXT - text output, useful for LLMs that have poor support for structured output, e.g. when using tiny (and dumb) LLMs for testing.
responseFormatType enum {JSON, TEXT} default=JSON

# Defines what to do when the response is not in the expected format.
# DISCARD - discard the response and return null.
# WARN - discard the response, return null and log a warning.
# FAIL - discard the request and raise an error, recommend
invalidResponseFormatPolicy enum {DISCARD, WARN, FAIL} default=DISCARD