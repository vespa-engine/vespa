# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
package=ai.vespa.llm.clients

# The LLM model to use
model model

# Maximum number of requests to handle in parallel pr container node
parallelRequests int default=1

# Additional number of requests to put in queue for processing before starting to reject new requests
maxQueueSize int default=100

# Max number of milliseconds to wait in the queue before rejecting a request
maxQueueWait int default=10000

# Use GPU
useGpu bool default=true

# Maximum number of model layers to run on GPU
gpuLayers int default=1000000

# Number of threads to use for CPU processing - -1 means use all available cores
# Not used for GPU processing
threads int default=-1

# Context size for the model
# Context is divided between parallel requests. So for 10 parallel requests, each "slot" gets 1/10 of the context
contextSize int default=4096

# Maximum number of tokens to process in one request - overridden by inference parameters
maxTokens int default=512

# Specifies what to do when LLM context size is too small to process prompt and completion tokens.
# NONE ignore context size and runs inference no matter prompt and completion size. 
# It can lead to nonsense output and performance issues.
# To avoid this, application developers can adjust prompts and maxTokens to fit into contextSize.
# SKIP skip inference for prompts when context size is too small for prompt + completion tokens.
# ERROR throws an exception "Context size is too small ..."
contextSizeManagement enum {NONE, SKIP, ERROR} default=NONE