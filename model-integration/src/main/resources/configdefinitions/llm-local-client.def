# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
package=ai.vespa.llm.clients

# The LLM model to use
model model

# Maximum number of requests to handle in parallel pr container node
parallelRequests int default=1

# Additional number of requests to put in queue for processing before starting to reject new requests
maxQueueSize int default=100

# Max number of milliseconds to wait in the queue before rejecting a request
maxQueueWait int default=10000

# Use GPU
useGpu bool default=true

# Maximum number of model layers to run on GPU
gpuLayers int default=1000000

# Number of threads to use for CPU processing - -1 means use all available cores
# Not used for GPU processing
threads int default=-1

# Context size for the model
# Context is divided between parallel requests. So for 10 parallel requests, each "slot" gets 1/10 of the context
contextSize int default=4096

# Maximum number of tokens to generate in one request - can be overridden by inference parameters
maxTokens int default=512

# Specifies number of tokens to truncate prompts to, should be > 0
# 0 or less means that prompts will not be truncated.  
# To avoid nonsense completions: maxPromptTokens + maxTokens <= contextSize
# To increase parallelism: maxPromptTokens + maxTokens -> contextSize / parallelRequests
maxPromptTokens int default=0

# Specifies what to do when prompt + maxTokens > contextSize.
# NONE - default option, runs inference no matter context and prompt size.
# In this case, newer tokens overwrite older ones and the model looses access to part of the prompt and generated tokens. 
# It might lead to nonsense completions and performance issues.
# To avoid this, set maxPromptTokens and maxTokens so that maxPromptTokens + maxTokens <= contextSize.
# SKIP - skip inference when prompt + maxTokens > contextSize
# ERROR - return exception "Context size is too small ..."
contextOverflowPolicy enum {NONE, SKIP, ERROR} default=NONE